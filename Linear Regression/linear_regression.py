# -*- coding: utf-8 -*-
"""Linear Regression

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rySs2TPliviuupk628oCfCdbuJL7-hf4
"""

import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt

class linear_regression:
    def __init__(self, trainx: np.ndarray, trainy: np.ndarray, testx: np.ndarray, testy: np.ndarray, initialize = "zero"):
        self.trainx = trainx
        self.trainy = trainy
        self.testx = testx
        self.testy = testy
        self.n_sample, self.n_dim = trainx.shape
        self.optimal = self._optimal()
        if initialize == "zero":
            self.w = np.zeros([self.n_dim+1, 1])

    def grad(self, w, x, y):
        sample = y.shape[0]
        return -np.matmul(np.c_[np.ones(sample), x].transpose(), 
                            (y - np.matmul(np.c_[np.ones(sample), x], w).reshape(-1)).reshape([-1,1]))
    
    def loss(self, x, y):
        sample_loss = y.shape[0]
        return np.sum(np.square(y - np.matmul(np.c_[np.ones(sample_loss), x],  self.w).reshape(-1)))

    def StochasticGradient(self, r = 0.009, convergence = 10**-6):
        loss = []
        test_loss = []
        sample_SG = 1
        
        for _ in range(100000):
            index = np.random.choice(np.arange(self.n_sample), sample_SG, replace=False)
            x = self.trainx[index,:].reshape( [sample_SG, -1] )
            y = self.trainy[index]
            
            grad = -np.matmul(np.c_[np.ones(sample_SG), x].transpose(), 
                                    (y - np.matmul(np.c_[np.ones(sample_SG), x], self.w).reshape(-1)).reshape([-1,1]))
           
            new_w = self.w - r * grad
    
            if np.linalg.norm(new_w - self.w) < convergence:
                self.w = new_w
                loss.append(self.loss(self.trainx, self.trainy))
                test_loss.append(self.loss(self.testx, self.testy))
                break
            else:
                self.w = new_w
                loss.append(self.loss(self.trainx, self.trainy))
                test_loss.append(self.loss(self.testx, self.testy))
        return loss, test_loss

    def BatchGradient(self, r = 0.001, convergence = 10**-6):
        loss = []
        test_loss = []
        for _ in range(1000):
            grad = -np.matmul(np.c_[np.ones(self.n_sample), self.trainx].transpose(), 
                                    (self.trainy - np.matmul(np.c_[np.ones(self.n_sample), self.trainx], self.w).reshape(-1)).reshape([-1,1]))/self.n_sample
            
            new_w = self.w - r * grad
            if np.linalg.norm(new_w - self.w) < convergence:
                self.w = new_w
                loss.append(self.loss(self.trainx, self.trainy))
                test_loss.append(self.loss(self.testx, self.testy))
                break
            else:
                self.w = new_w
                loss.append(self.loss(self.trainx, self.trainy))
                test_loss.append(self.loss(self.testx, self.testy))
        return loss, test_loss
    
    def _optimal(self,):
        x = np.c_[np.ones(self.n_sample), self.trainx].transpose()
        y = self.trainy
        return np.matmul(np.matmul(np.linalg.inv(np.matmul(x, x.transpose())), x), y)
    
    def predict(self, testx):
        return np.matmul(np.c_[np.ones(self.n_sample), testx], self.w)

def error_rate(predy,y):
    
    return np.mean(np.reshape(predy,(-1,1))!=np.reshape(y,(-1,1)))

colnames = ['Cement', 'Slag', 'Fly_ash', 'Water', 'SP', 'Coarse_Aggr', 'Fine_Aggr', 'y']

train_data = pd.read_csv('train.csv', header=None, names=colnames) 
test_data = pd.read_csv('test.csv', header=None, names=colnames) 

regressor1 = linear_regression(trainx = train_data[train_data.columns.to_numpy(copy = True)[:-1]].to_numpy(copy = True), trainy = train_data.y.to_numpy(copy = True), testx = test_data[train_data.columns.to_numpy(copy = True)[:-1]].to_numpy(copy = True), testy = test_data.y.to_numpy(copy = True))
StochasticGradient_loss, StochasticGradient_test_loss = regressor1.StochasticGradient(r = 0.005, convergence = pow(10,-6)) 
regressor2 = linear_regression(trainx = train_data[train_data.columns.to_numpy(copy = True)[:-1]].to_numpy(copy = True), trainy = train_data.y.to_numpy(copy = True), testx = test_data[train_data.columns.to_numpy(copy = True)[:-1]].to_numpy(copy = True), testy = test_data.y.to_numpy(copy = True))
BatchGradient_loss, BatchGradient_test_loss = regressor2.BatchGradient(r = 0.05, convergence = pow(10,-6))

plt.plot(BatchGradient_loss)

plt.title('Batch Gradient')
plt.ylabel('loss')
plt.xlabel('i')
plt.show()

print("Cost Function changes ", regressor2.w)
print("Test Loss ", BatchGradient_test_loss[-1])
print("Learning rate: 0.05")
print("----------------------------------------------------------")
plt.plot(StochasticGradient_loss)
plt.title('Stochastic Gradient')
plt.ylabel('loss')
plt.xlabel('i')
plt.show()
print("Cost Function changes ", regressor1.w)
print("Test Loss ", StochasticGradient_test_loss[-1])
print("Learning rate: 0.005")
print("----------------------------------------------------------")
print("Optimal Weight Vector: ", regressor1.optimal)